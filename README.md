
---

# **The Testing Showdown: Humans vs. LLMs in Bug Detection**

**TEAM 19 — SSD Final Project**
Software Engineering Research Centre (SERC), IIIT Hyderabad

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![pytest](https://img.shields.io/badge/testing-pytest-green.svg)](https://pytest.org/)
[![hypothesis](https://img.shields.io/badge/testing-hypothesis-orange.svg)](https://hypothesis.readthedocs.io/)

---

# **Project Overview**

This project empirically compares **LLM-generated unit tests** vs **human-written property-based tests** for detecting subtle bugs.
We curated a *Bug Portfolio* of **30 intentionally buggy Python functions** (from the MBPP dataset) and executed two competing test suites:

* **LLM Challenger:** Example-based tests generated by ChatGPT/DeepSeek
* **Human Defender:** Property-based tests using Hypothesis

We analyze detection performance, bug taxonomies, failure modes, and the complementary strengths of both approaches.

### **Research Question**

**Which testing strategy finds more bugs, and what types of bugs are each method uniquely good at detecting?**

---

# **Team Members**

* Om Mehra — 2025201008
* Hardik Kothari — 2025201046
* Ankit Chavda — 2025201045
* M V Ramana Murthy — 2025204034
* P Chaitanya Pavan Kumar — 2025204023

Advisor: **Abhishek Singh**

---

# **Project Objectives**

1. **Bug Portfolio Creation**

   * 30 buggy versions of MBPP functions, each containing a single subtle, realistic bug.

2. **Dual Testing Strategy**

   * *LLM-generated pytest suites*
   * *Human-written Hypothesis property tests*

3. **Comparative Evaluation**

   * Measure detection counts, run time, coverage patterns, and failure categories.

4. **Research Insights**

   * Identify why certain bugs evade both methods
   * Propose a new failure taxonomy
   * Offer engineering guidelines for hybrid testing strategies.

---

# **Repository Structure**

```
ssd-final-project/
├── bug_portfolio/              
│   ├── *_buggy.py             
│   ├── *_correct.py           
│   └── metadata.jsonl         
│
├── human_tests/               
│   └── test_*.py              
│
├── llm_tests/
│   ├── generated_tests/       
│   │   └── test_*.py
│   └── prompts/               
│
├── Report.ipynb               
├── mbpp.jsonl                 
├── helper.py                  
│
├── results_summary.csv        
├── final_scorecard.csv        
│
├── TEAM_19_REPORT.pdf         
├── TEAM_19_PHASE2_REPORT.md   
└── README.md                  
```

---

# **Quick Start**

### Installation

```bash
git clone https://github.com/omii2403/ssd-final-project.git
cd ssd-final-project
pip install pandas matplotlib pytest hypothesis tabulate
```

### Running Tests

```bash
pytest human_tests/ -v
pytest llm_tests/generated_tests/ -v
```

### Full Analysis (Recommended)

```bash
jupyter notebook Report.ipynb
# Run all cells
```

---

# **Bug Taxonomy (Distribution)**

*(Summarized from 30 buggy functions)*

| Category                       | Count |   % |
| ------------------------------ | ----: | --: |
| Comparator / inequality errors |     7 | 23% |
| Off-by-one / loop-bound errors |     7 | 23% |
| Boundary / termination issues  |     2 |  7% |
| Missing base/special case      |     2 |  7% |
| Indexing mistakes              |     4 | 13% |
| Wrong formula / constant       |     3 | 10% |
| Wrong assignment operation     |     1 |  3% |
| Wrong parameter                |     1 |  3% |
| Regex typo                     |     1 |  3% |
| Initialization error           |     1 |  3% |
| Increment typo                 |     1 |  3% |

**Key Insight:** Comparator + off-by-one bugs dominate (≈46%), aligning with common mutation patterns in real-world software.

---

# **Representative Examples Mapped to Patterns**

### **Missed by LLM but found by humans**

* `find_Max` — strict vs non-strict recursion
* `first_Digit` — boundary `fact == 10`
* `sum_Of_Primes`, `sum_of_odd_Factors` — number-theory semantics
* `sort_by_dnf` — increment by 2 instead of 1

### **Missed by humans but found by LLM**

* `bitonic_subsequence` — LIS ≥ vs >
* `is_subset` — loop starting index
* `longest_increasing_subsequence` — wrong loop bounds

### **Both methods caught**

* Clear boundary and structural mistakes (`set_Right_most_Unset_Bit`, `sumofFactors`, `pass_validity`, etc.)

### **Neither method caught**

*(Requires deeper semantic or pathological inputs)*

* `find_max_val`
* `max_chain_length`
* `find_first_occurrence`
* `find_longest_conseq_subseq`

---

# **New Failure Taxonomy (Undetected Bugs)**

We discovered **four bugs that neither LLM nor human tests found**, leading to a new research-relevant taxonomy.

| Failure Category                                           | Description                                                                               | Undetected Bugs                                       |
| ---------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| **Requires semantic understanding**                        | Depends on intended meaning of solution (not visible from code structure).                | `find_max_val`, `max_chain_length`                    |
| **Requires domain knowledge**                              | Depends on algorithmic invariants (e.g., binary search correctness, duplicate semantics). | `find_first_occurrence`, `find_longest_conseq_subseq` |
| **Requires very large input**                              | Only triggers at high input scales.                                                       | *None in dataset*                                     |
| **Needs advanced random input beyond Hypothesis defaults** | Requires rare structured patterns not usually generated.                                  | `find_first_occurrence`, `find_longest_conseq_subseq` |
| **Rare pathological edge-case**                            | Needs extremely specific input (e.g., only `0` is valid).                                 | `find_max_val`, `max_chain_length`                    |

**Research Insight:** These categories reveal structural blind spots in both LLM and human-generated tests.

---

# **Final Insights: Where LLMs Shine, Where Humans Shine**

| Situation                                    | Better Method    | Reason                                                         |
| -------------------------------------------- | ---------------- | -------------------------------------------------------------- |
| **Edge case reasoning**                      | **Hypothesis**   | Randomized exploration uncovers boundary & off-by-one errors.  |
| **Semantic bugs**                            | **Humans / LLM** | Requires understanding algorithm intent, not just execution.   |
| **Numerical precision / large search space** | **Hypothesis**   | Expands numeric space far beyond manually written tests.       |
| **Input format validation**                  | **LLM**          | LLMs reason about structural constraints and malformed inputs. |

**Conclusion:**
Neither method dominates universally—each catches bugs the other misses. The strongest strategy is **Hybrid Testing**, combining:

* LLM *generative breadth*
* Hypothesis *random diversity*
* Human *semantic insight*

---

# **Conclusion**

* **Humans detected:** 24/30 bugs
* **LLM tests detected:** 21/30 bugs
* **Both detected:** 19 bugs
* **Neither detected:** 4 bugs

Humans narrowly win, but the real strength lies in combining both approaches.
LLMs excel at broad coverage and structural intuition; humans excel at semantic reasoning and conceptual invariants.

---
