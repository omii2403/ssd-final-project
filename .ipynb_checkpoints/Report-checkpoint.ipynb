{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate\n",
    "# !pip install pytest tabulate hypothesis pandas matplotlib\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f01f16-9ada-45b2-9433-9a05639055b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "with open(\"bug_portfolio/metadata.jsonl\", \"r\") as f:\n",
    "    buffer = \"\"\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        buffer += line\n",
    "        if line.endswith(\"}\"):\n",
    "            try:\n",
    "                metadata.append(json.loads(buffer))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            buffer = \"\"\n",
    "\n",
    "bug_map = {item[\"name\"]: item.get(\"bug_description\", \"No description provided\") for item in metadata}\n",
    "functions = [item[\"name\"] for item in metadata]\n",
    "\n",
    "# Create mapping from function name to bug description\n",
    "bug_map = {item[\"name\"]: item.get(\"bug_description\", \"No description provided\") for item in metadata}\n",
    "functions = [item[\"name\"] for item in metadata]\n",
    "\n",
    "print(functions)\n",
    "print(len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6c02b-467f-47e6-84ae-b29c26f90424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_counts_from_output(out):\n",
    "    \"\"\"Return (passed, failed) parsed from pytest stdout.\"\"\"\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    m_passed = re.search(r\"(\\d+)\\s+passed\", out)\n",
    "    m_failed = re.search(r\"(\\d+)\\s+failed\", out)\n",
    "    if m_passed:\n",
    "        passed = int(m_passed.group(1))\n",
    "    if m_failed:\n",
    "        failed = int(m_failed.group(1))\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def run_tests(file):\n",
    "    \"\"\"\n",
    "    Run pytest on a single file.\n",
    "    Returns:\n",
    "        exitcode, passed, failed, time_taken_seconds, stdout, stderr\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(os.path.basename(file))[0]\n",
    "    json_file = f\"report_{base}.json\"\n",
    "\n",
    "    # ---- attempt 1: with JSON ----\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    cmd = [\"pytest\", \"-q\", file, \"--json-report\", f\"--json-report-file={json_file}\"]\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "    time_taken = time.perf_counter() - start\n",
    "\n",
    "    # If pytest did not generate JSON or errored out → fallback\n",
    "    if proc.returncode == 4 or not os.path.exists(json_file):\n",
    "        # ---- fallback: run WITHOUT json-report ----\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        fallback_cmd = [\"pytest\", \"-q\", file]\n",
    "        proc2 = subprocess.run(fallback_cmd, capture_output=True, text=True)\n",
    "\n",
    "        time_taken = time.perf_counter() - start\n",
    "\n",
    "        passed, failed = parse_counts_from_output(proc2.stdout)\n",
    "        return proc2.returncode, passed, failed, time_taken, proc2.stdout, proc2.stderr\n",
    "\n",
    "    # ---- JSON successfully created: extract summary ----\n",
    "    try:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        summary = data.get(\"summary\", {})\n",
    "        passed = summary.get(\"passed\", 0)\n",
    "        failed = summary.get(\"failed\", 0)\n",
    "    except Exception:\n",
    "        # fallback to parsing stdout\n",
    "        passed, failed = parse_counts_from_output(proc.stdout)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(json_file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    return proc.returncode, passed, failed, time_taken, proc.stdout, proc.stderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c47baa-499b-4d27-a085-efb513a09503",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_found = {}\n",
    "human_found = {}\n",
    "\n",
    "for func in functions:\n",
    "    print(f\"\\n=== Testing `{func}` ===\")\n",
    "    print(f\"Bug Description: {bug_map[func]}\")\n",
    "\n",
    "    # LLM tests\n",
    "    llm_file = f\"llm_tests/generated_tests/test_{func}.py\"\n",
    "    llm_exitcode, llm_passed, llm_failed, llm_time, llm_out, llm_err = run_tests(llm_file)\n",
    "\n",
    "    llm_status = \"found\" if llm_exitcode != 0 else \"not found\"\n",
    "    llm_found[func] = (llm_exitcode != 0)\n",
    "    print(f\"LLM tests for `{func}`: Bug {llm_status} (exit code {llm_exitcode})\")\n",
    "    print(f\"LLM Summary: {llm_passed} passed, {llm_failed} failed (time: {llm_time:.2f}s)\")\n",
    "\n",
    "    # Human tests\n",
    "    human_file = f\"human_tests/test_{func}.py\"\n",
    "    human_exitcode, human_passed, human_failed, human_time, human_out, human_err = run_tests(human_file)\n",
    "\n",
    "    human_status = \"found\" if human_exitcode != 0 else \"not found\"\n",
    "    human_found[func] = (human_exitcode != 0)\n",
    "    print(f\"Human tests for `{func}`: Bug {human_status} (exit code {human_exitcode})\")\n",
    "    print(f\"Human Summary: {human_passed} passed, {human_failed} failed (time: {human_time:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e22c90-6fe8-41f7-b563-9a695bb6a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for func in functions:\n",
    "    results.append({\n",
    "        \"Function\": func,\n",
    "        \"LLM Found\": \"Yes\" if llm_found.get(func, False) else \"No\",\n",
    "        \"Human Found\": \"Yes\" if human_found.get(func, False) else \"No\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# Manually update any function’s results here:\n",
    "# df.loc[df['Function'] == 'gold_mine_problem', 'LLM Found'] = 'No'\n",
    "# df.loc[df['Function'] == 'set_Right_most_Unset_Bit', 'Human Found'] = 'Yes'\n",
    "# Uncomment and edit as needed\n",
    "\n",
    "# ---- Manual overrides go here ----\n",
    "# df.loc[df['Function'] == 'gold_mine_problem', 'LLM Found'] = 'No'\n",
    "# df.loc[df['Function'] == 'set_Right_most_Unset_Bit', 'Human Found'] = 'Yes'\n",
    "# ----------------------------------\n",
    "\n",
    "print(\"### Individual Test Results\\n\")\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf606f-f99c-4972-8e1f-b0ae5eb4ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_llm = ((df[\"LLM Found\"] == \"Yes\") & (df[\"Human Found\"] == \"No\")).sum()\n",
    "only_human = ((df[\"LLM Found\"] == \"No\") & (df[\"Human Found\"] == \"Yes\")).sum()\n",
    "both = ((df[\"LLM Found\"] == \"Yes\") & (df[\"Human Found\"] == \"Yes\")).sum()\n",
    "neither = ((df[\"LLM Found\"] == \"No\") & (df[\"Human Found\"] == \"No\")).sum()\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "# Overall detection percentages\n",
    "llm_total_found = only_llm + both\n",
    "human_total_found = only_human + both\n",
    "\n",
    "llm_overall_percentage = round((llm_total_found / total) * 100, 2)\n",
    "human_overall_percentage = round((human_total_found / total) * 100, 2)\n",
    "\n",
    "scorecard = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Bugs found only by LLM tests\",\n",
    "        \"Bugs found only by Human properties\",\n",
    "        \"Bugs found by both\",\n",
    "        \"Bugs found by neither\"\n",
    "    ],\n",
    "    \"Count\": [only_llm, only_human, both, neither],\n",
    "})\n",
    "\n",
    "# Add percentage column\n",
    "scorecard[\"Percentage\"] = (scorecard[\"Count\"] / total * 100).round(2).astype(str) + \"%\"\n",
    "\n",
    "print(\"\\n\\n### Final Scorecard\\n\")\n",
    "print(scorecard.to_markdown(index=False))\n",
    "\n",
    "print(\"\\n### Overall Detection Rates\\n\")\n",
    "print(f\"LLM Overall Detection: {llm_total_found}/{total} = {llm_overall_percentage}%\")\n",
    "print(f\"Human Overall Detection: {human_total_found}/{total} = {human_overall_percentage}%\")\n",
    "\n",
    "# Save CSVs\n",
    "df.to_csv(\"results_summary.csv\", index=False)\n",
    "scorecard.to_csv(\"final_scorecard.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338336e-7fe7-4ca8-acd7-0d3ae341d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(scorecard[\"Metric\"], scorecard[\"Count\"])\n",
    "plt.title(\"Bug Detection Summary (LLM vs Human)\")\n",
    "plt.ylabel(\"Number of Bugs Found\")\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe446c3-3158-4f63-ba01-8c8fff1a1127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6769ead-d982-428f-855a-6f6a0673764d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a618818-c94b-4daf-9ab8-01a0dbfddb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
