{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80704b9a-e1da-4060-b448-8a39873e20d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (8.4.2)\n",
      "Requirement already satisfied: hypothesis in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.143.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest) (2.19.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hypothesis) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in c:\\users\\ommeh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from hypothesis) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install testing libraries (pytest and hypothesis).\n",
    "!pip install pytest hypothesis\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "\n",
    "# Change working directory to the project root if needed\n",
    "# os.chdir('path/to/project_directory')\n",
    "\n",
    "# File paths for LLM-generated tests and human (Hypothesis) tests\n",
    "llm_tests = {\n",
    "    \"decimal_to_binary\":  \"llm_tests/generated_tests/test_decimal_to_binary_llm.py\",\n",
    "    \"get_max_gold\":       \"llm_tests/generated_tests/test_get_max_gold_llm.py\",\n",
    "    \"set_Right_most_Unset_Bit\": \"llm_tests/generated_tests/test_set_Right_most_Unset_Bit_llm.py\"\n",
    "}\n",
    "human_tests = {\n",
    "    \"decimal_to_binary\":  \"human_tests/test_decimal_to_binary_props.py\",\n",
    "    \"get_max_gold\":       \"human_tests/test_get_max_gold_props.py\",\n",
    "    \"set_Right_most_Unset_Bit\": \"human_tests/test_set_Right_most_Unset_Bit_props.py\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ec53bc-f48c-42a5-8bd3-1561bba9185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                                   [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[1-1]\u001b[0m - assert 10 == 1\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[2-10]\u001b[0m - assert 100 == 10\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[3-11]\u001b[0m - assert 110 == 11\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[4-100]\u001b[0m - assert 1000 == 100\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[5-101]\u001b[0m - assert 1010 == 101\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[7-111]\u001b[0m - assert 1110 == 111\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[8-1000]\u001b[0m - assert 10000 == 1000\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[15-1111]\u001b[0m - assert 11110 == 1111\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[16-10000]\u001b[0m - assert 100000 == 10000\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[31-11111]\u001b[0m - assert 111110 == 11111\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_decimal_to_binary[255-11111111]\u001b[0m - assert 111111110 == 11111111\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_large_number\u001b[0m - AssertionError: assert 11111111110 == 1111111111\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_decimal_to_binary_llm.py::\u001b[1mtest_powers_of_two_pattern\u001b[0m - assert 10 == 1\n",
      "\u001b[31m\u001b[31m\u001b[1m13 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.59s\u001b[0m\u001b[0m\n",
      "\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                                               [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m human_tests/test_decimal_to_binary_props.py::\u001b[1mtest_binary_roundtrip\u001b[0m - AssertionError: assert 2 == 1\n",
      "\u001b[31mFAILED\u001b[0m human_tests/test_decimal_to_binary_props.py::\u001b[1mtest_monotonic_increase\u001b[0m - AssertionError: assert 2 == 1\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m\u001b[31m in 1.73s\u001b[0m\u001b[0m\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                                        [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_get_max_gold_llm.py::\u001b[1mtest_single_column\u001b[0m - assert 5 == 9\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_get_max_gold_llm.py::\u001b[1mtest_two_rows_mix\u001b[0m - assert 201 == 300\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m7 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                                            [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m human_tests/test_get_max_gold_props.py::\u001b[1mtest_single_column_mine\u001b[0m - assert 0 == 1\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 1.43s\u001b[0m\u001b[0m\n",
      "\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                                    [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m llm_tests/generated_tests/test_set_Right_most_Unset_Bit_llm.py::\u001b[1mtest_set_right_most_unset_bit[0-1]\u001b[0m - assert 0 == 1\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m12 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                                            [100%]\u001b[0m\n",
      "\u001b[36m\u001b[1m=============================================== short test summary info ===============================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m human_tests/test_set_Right_most_Unset_Bit_props.py::\u001b[1mtest_edge_cases\u001b[0m - assert 0 == 1\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.29s\u001b[0m\u001b[0m\n",
      "Function `decimal_to_binary`: LLM found bug? True, Human found bug? True\n",
      "Function `get_max_gold`: LLM found bug? True, Human found bug? True\n",
      "Function `set_Right_most_Unset_Bit`: LLM found bug? True, Human found bug? True\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for func in [\"decimal_to_binary\", \"get_max_gold\", \"set_Right_most_Unset_Bit\"]:\n",
    "    # Run LLM-generated pytest suite on the buggy implementation\n",
    "    llm_test_path = llm_tests[func]\n",
    "    exit_code_llm = pytest.main([\"-q\", \"--disable-warnings\", \"--tb=no\", llm_test_path])\n",
    "\n",
    "    # Run Hypothesis-based pytest suite on the buggy implementation\n",
    "    human_test_path = human_tests[func]\n",
    "    exit_code_human = pytest.main([\"-q\", \"--disable-warnings\", \"--tb=no\", human_test_path])\n",
    "\n",
    "    # Determine if each strategy found the bug\n",
    "    found_llm = (exit_code_llm != 0)\n",
    "    found_human = (exit_code_human != 0)\n",
    "    results[func] = {\"LLM_found\": found_llm, \"Human_found\": found_human}\n",
    "\n",
    "# Print summary of results for each function\n",
    "for func, res in results.items():\n",
    "    print(f\"Function `{func}`: LLM found bug? {res['LLM_found']}, Human found bug? {res['Human_found']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e22c90-6fe8-41f7-b563-9a695bb6a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Scorecard:\n",
      "- Bugs found *only* by LLM tests: 0\n",
      "- Bugs found *only* by Human tests: 0\n",
      "- Bugs found by *both* methods: 3\n",
      "- Bugs found by *neither* method: 0\n"
     ]
    }
   ],
   "source": [
    "llm_only = human_only = both = neither = 0\n",
    "for res in results.values():\n",
    "    llm = res[\"LLM_found\"]\n",
    "    human = res[\"Human_found\"]\n",
    "    if llm and human:\n",
    "        both += 1\n",
    "    elif llm and not human:\n",
    "        llm_only += 1\n",
    "    elif human and not llm:\n",
    "        human_only += 1\n",
    "    else:\n",
    "        neither += 1\n",
    "\n",
    "# Print the final scorecard\n",
    "print(\"\\nFinal Scorecard:\")\n",
    "print(f\"- Bugs found *only* by LLM tests: {llm_only}\")\n",
    "print(f\"- Bugs found *only* by Human tests: {human_only}\")\n",
    "print(f\"- Bugs found by *both* methods: {both}\")\n",
    "print(f\"- Bugs found by *neither* method: {neither}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcf606f-f99c-4972-8e1f-b0ae5eb4ca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Strategy | Bugs Found |\n",
      "|----------|------------|\n",
      "| LLM only | 0 |\n",
      "| Human only | 0 |\n",
      "| Both | 3 |\n",
      "| Neither | 0 |\n"
     ]
    }
   ],
   "source": [
    "# Optional: create a simple markdown table for clarity\n",
    "print(\"\\n| Strategy | Bugs Found |\")\n",
    "print(\"|----------|------------|\")\n",
    "print(f\"| LLM only | {llm_only} |\")\n",
    "print(f\"| Human only | {human_only} |\")\n",
    "print(f\"| Both | {both} |\")\n",
    "print(f\"| Neither | {neither} |\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338336e-7fe7-4ca8-acd7-0d3ae341d813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
